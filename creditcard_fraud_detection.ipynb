{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Btere/btereml/blob/main/creditcard_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XATPfMARQxM3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcehiOAKQxQK"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALNpjxULQxTn"
      },
      "outputs": [],
      "source": [
        "def read_csv_files(dataset_path: Path)-> pd.DataFrame:\n",
        "    train_dataset = pd.read_csv(f'{dataset_path}/train_fraud.csv', index_col=False)\n",
        "    test_dataset = pd.read_csv(f'{dataset_path}/test_fraud.csv', index_col=False)\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWR7GWaMQxWM"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = read_csv_files(DATASET_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utuP-JAZSHQp"
      },
      "outputs": [],
      "source": [
        "display(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZXq9VDkVhpr"
      },
      "outputs": [],
      "source": [
        "# Dataset overview\n",
        "\n",
        "train_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-61cqdY2VhnY"
      },
      "outputs": [],
      "source": [
        "test_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0l1zujCWAGa"
      },
      "outputs": [],
      "source": [
        "train_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nm8FBQ-lV1nF"
      },
      "outputs": [],
      "source": [
        "train_dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkkdPJGLWGdD"
      },
      "outputs": [],
      "source": [
        "train_dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu_cZhIcWGLh"
      },
      "outputs": [],
      "source": [
        "train_dataset.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9cuVGjISHUh"
      },
      "outputs": [],
      "source": [
        "display(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D_tU-B2SHXb"
      },
      "outputs": [],
      "source": [
        "test_dataset.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9RUbLd-SGFx"
      },
      "outputs": [],
      "source": [
        "train_dataset[\"is_fraud\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBmIGu2eSGId"
      },
      "outputs": [],
      "source": [
        "test_dataset[\"is_fraud\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rk8Xdu4cizL"
      },
      "source": [
        "Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS-BWgyLYJw7"
      },
      "outputs": [],
      "source": [
        "train_data = train_dataset.drop(columns='Unnamed: 0')\n",
        "test_data = test_dataset.drop(columns='Unnamed: 0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J37kDp4wFDNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s96EC69dSBSR"
      },
      "outputs": [],
      "source": [
        "train_data[\"dob\"] = pd.to_datetime(train_data[\"dob\"])\n",
        "train_data['trans_date_trans_time'] = pd.to_datetime(train_data['trans_date_trans_time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5eBD9XvSBVM"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rzldSV2fsbY"
      },
      "outputs": [],
      "source": [
        "test_data[\"dob\"] = pd.to_datetime(test_data[\"dob\"])\n",
        "test_data['trans_date_trans_time'] = pd.to_datetime(test_data['trans_date_trans_time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVUyeSM-di_R"
      },
      "outputs": [],
      "source": [
        "train_data.shape , test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVkvXOoIdjB2"
      },
      "outputs": [],
      "source": [
        "train_data.columns , test_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS6qugSGfBw_"
      },
      "source": [
        "First, we want to apply some transformation to the dataset to normalize the features values before encoding the categorical labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFqXZG8ApEMH"
      },
      "source": [
        "counts the number of occurrences of each job title among the rows in the test_data DataFrame where the is_fraud column is 1. It helps in understanding the distribution of job titles specifically for fraudulent cases in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCR5Dg0Yod0V"
      },
      "outputs": [],
      "source": [
        "test_data[test_data[\"is_fraud\"] == 1][\"job\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNaCZTqAoOfF"
      },
      "outputs": [],
      "source": [
        "train_data[train_data[\"is_fraud\"] == 1][\"merchant\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNiYbEscgd7O"
      },
      "outputs": [],
      "source": [
        "# encoding test data\n",
        "\"\"\"\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "\n",
        "test_data['merchant']=encoder.fit_transform(test_data['merchant'])\n",
        "test_data['category']=encoder.fit_transform(test_data['category'])\n",
        "test_data['street']=encoder.fit_transform(test_data['street'])\n",
        "test_data['job']=encoder.fit_transform(test_data['job'])\n",
        "test_data['trans_num']=encoder.fit_transform(test_data['trans_num'])\n",
        "test_data['first']=encoder.fit_transform(test_data['first'])\n",
        "test_data['city']=encoder.fit_transform(test_data['city'])\n",
        "test_data['state']=encoder.fit_transform(test_data['state'])\n",
        "test_data['last']=encoder.fit_transform(test_data['last'])\n",
        "test_data['gender']=encoder.fit_transform(test_data['gender'])\n",
        "test_data['trans_date_trans_time']=encoder.fit_transform(test_data['trans_date_trans_time'])\n",
        "test_data['dob']=encoder.fit_transform(test_data['dob'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZl9H5GZgk_c"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Train dataset and applying some normalization to the train and test set."
      ],
      "metadata": {
        "id": "EgLZ-9KPWEkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()\n"
      ],
      "metadata": {
        "id": "MEdCONEEdFMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_copy = train_data.copy()\n",
        "test_data_copy = test_data.copy()"
      ],
      "metadata": {
        "id": "NyiB1PWxyxdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
        "    \"\"\"Encoding categorical columns and applying scaling and normalization\"\"\"\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    numerical_cols = df.select_dtypes(include=['number']).columns.drop(target_col)\n",
        "\n",
        "    # Define transformations for numerical and categorical features\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('minmax', MinMaxScaler()),   # Normalize numerical data\n",
        "        ('standard', StandardScaler())  # Scale numerical data\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = OneHotEncoder(drop='first')  # One-Hot Encode categorical data\n",
        "\n",
        "    # Combine transformations using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Apply transformations\n",
        "    df_processed = preprocessor.fit_transform(df)\n",
        "\n",
        "    # Convert the transformed data back to a DataFrame\n",
        "    df_processed = pd.DataFrame(df_processed, columns=preprocessor.get_feature_names_out())\n",
        "\n",
        "    # Add the target column back to the DataFrame\n",
        "    df_processed[target_col] = df[target_col].values\n",
        "\n",
        "    return df_processed\n",
        "\n"
      ],
      "metadata": {
        "id": "bIIIkS-sempj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xroIM6yBjRXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
        "    \"\"\"Encoding categorical columns and applying scaling and normalization\"\"\"\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    standard_scaler = StandardScaler()\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "\n",
        "    # Encode categorical columns first to numerical\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for column in categorical_cols:\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "\n",
        "    # Handle datetime columns\n",
        "    datetime_cols = df.select_dtypes(include=['datetime']).columns\n",
        "    for column in datetime_cols:\n",
        "        df[column + '_year'] = df[column].dt.year\n",
        "        df[column + '_month'] = df[column].dt.month\n",
        "        df[column + '_day'] = df[column].dt.day\n",
        "        df[column + '_hour'] = df[column].dt.hour\n",
        "        df[column + '_minute'] = df[column].dt.minute\n",
        "        df[column + '_second'] = df[column].dt.second\n",
        "\n",
        "    df = df.drop(columns=datetime_cols)\n",
        "\n",
        "    # Now the dataset only contains numerical data, so we apply scaling and normalization to all columns except target_col\n",
        "    numerical_cols = df.select_dtypes(include=['number']).columns.drop(target_col)  # Exclude the target column\n",
        "\n",
        "    # First, apply MinMaxScaler for normalization\n",
        "    df[numerical_cols] = minmax_scaler.fit_transform(df[numerical_cols])\n",
        "    # Then, apply StandardScaler for scaling\n",
        "    df[numerical_cols] = standard_scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "TEoyq-DQP1av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
        "    \"\"\"encoding categorical columns and applying scaling and normalization\"\"\"\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    standard_scaler = StandardScaler()\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "\n",
        "    # Encode categorical columns first to numerical by selecting the columns\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for column in categorical_cols:\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "\n",
        "\n",
        "     # Handle datetime columns\n",
        "    datetime_cols = df.select_dtypes(include=['datetime']).columns\n",
        "    for column in datetime_cols:\n",
        "        df[column + '_year'] = df[column].dt.year\n",
        "        df[column + '_month'] = df[column].dt.month\n",
        "        df[column + '_day'] = df[column].dt.day\n",
        "        df[column + '_hour'] = df[column].dt.hour\n",
        "        df[column + '_minute'] = df[column].dt.minute\n",
        "        df[column + '_second'] = df[column].dt.second\n",
        "\n",
        "    df = df.drop(columns=datetime_cols)\n",
        "\n",
        "# Now the dataset only contains numerical data, so we apply scaling and normalization to all columns\n",
        "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
        "    numerical_cols = df.drop(target_col, axis=1)\n",
        "\n",
        "    # First, apply MinMaxScaler for normalization and then, apply StandardScaler for scaling\n",
        "    df[numerical_cols] = minmax_scaler.fit_transform(df[numerical_cols])\n",
        "    df[numerical_cols] = standard_scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "d69mktZCCHyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = preprocess_data(train_data, \"is_fraud\")\n",
        "#train_set.head(10)\n",
        "train_set[\"is_fraud\"].value_counts()"
      ],
      "metadata": {
        "id": "1tSpBlpNDHks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = preprocess_data(test_data_copy, \"is_fraud\")\n",
        "test_set.head(10)"
      ],
      "metadata": {
        "id": "sdTly9DBDtmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iApixowPNGn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.shape, test_set.shape"
      ],
      "metadata": {
        "id": "hBr-ZjlgDy-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GlwrsupRQNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.dtypes"
      ],
      "metadata": {
        "id": "I_EmLPLRfx31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWDJxbdtgIGm"
      },
      "outputs": [],
      "source": [
        "train_set.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrblarlagIRh"
      },
      "outputs": [],
      "source": [
        "test_set.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def corr(df: pd.DataFrame, column: str) -> None:\n",
        "  plt.figure(figsize = (10,10))\n",
        "  sns.heatmap(train_data.corr(), cmap = \"Reds\", annot = True, fmt = \".1f\")"
      ],
      "metadata": {
        "id": "iF8WZJzejJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corr(train_data, \"is_fraud\")"
      ],
      "metadata": {
        "id": "cjCuH02iBMYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting_set(training_set: pd.DataFrame, testing_set: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Check if 'is_fraud' is in both sets\n",
        "    if 'is_fraud' not in training_set.columns:\n",
        "        raise ValueError(\"'is_fraud' column missing in training_set\")\n",
        "    if 'is_fraud' not in testing_set.columns:\n",
        "        raise ValueError(\"'is_fraud' column missing in testing_set\")\n",
        "\n",
        "    # Split features and target\n",
        "    X_train = training_set.drop(columns='is_fraud')\n",
        "    y_train = training_set['is_fraud']\n",
        "\n",
        "\n",
        "    X_test = testing_set.drop(columns='is_fraud')\n",
        "    y_test = testing_set['is_fraud']\n",
        "\n",
        "    # Print shapes for verification\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"X_test shape:\", X_test.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RdI4agf-HDNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = splitting_set(train_set, test_set)"
      ],
      "metadata": {
        "id": "O3NUoZF1HHMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_test))"
      ],
      "metadata": {
        "id": "c1v3FG3_FKl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.columns.value_counts().sum()"
      ],
      "metadata": {
        "id": "AGCkVQKBCehz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train columns:\", train_set.columns)\n",
        "print(\"Test columns:\", test_set.columns)"
      ],
      "metadata": {
        "id": "g_9iGiz7HSTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert a Pandas DataFrame to a NumPy array, you can use the values attribute. This attribute returns a NumPy array containing the underlying data of the DataFrame. When you convert a Pandas DataFrame to a NumPy array, the rows and columns become rows and columns in the NumPy array.\n",
        "\n",
        "The structure remains the same, but the data type changes from a Pandas Series (for columns) to a NumPy array (for the entire DataFrame). This can be beneficial for certain operations that are more efficient in NumPy, such as numerical computations.\n",
        "\n",
        "\n",
        "It is a good practice to convert your train and test datast to numpy array before training the model with it.\n",
        "\n",
        "\n",
        "NumPy arrays are optimized for numerical operations, making them more efficient for training machine learning models compared to Pandas DataFrames.\n",
        "Many machine learning libraries, such as scikit-learn, expect the data to be in the form of NumPy arrays.\n",
        "Using NumPy arrays ensures consistency in your data format, making it easier to manage and analyze your data.\n",
        "\n"
      ],
      "metadata": {
        "id": "0pgDcmBPMmZw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLA-J85ojo7J"
      },
      "outputs": [],
      "source": [
        "#splitting train and test dataset and convert to a numpy array.\n",
        "\n",
        "#def splitting_set(training_set: pd.DataFrame, testing_set: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "\n",
        "  #X_train = training_set.loc[:, training_set.columns != 'is_fraud'].values\n",
        "  #y_train = training_set.loc[:, 'is_fraud'].values\n",
        "\n",
        "  #X_test = testing_set.loc[:, testing_set.columns != 'is_fraud'].values\n",
        "  #y_test = testing_set.loc[:, 'is_fraud'].values\n",
        "\n",
        "  #print(X_train.shape, y_train.shape)\n",
        "  #print(X_test.shape, y_test.shape)\n",
        "\n",
        "  #return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLreBF6yjpEr"
      },
      "outputs": [],
      "source": [
        "#X_train, y_train, X_test, y_test = splitting_set(train_set, test_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_train.shape, y_train.shape)\n",
        "#print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "R1_nWLFiF7Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKKVeP3-rIHN"
      },
      "outputs": [],
      "source": [
        "#model building and training\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "\n",
        "# Define all models\n",
        "logistic_regression = LogisticRegression()\n",
        "random_forest = RandomForestClassifier()\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "svc = SVC()\n",
        "knn = KNeighborsClassifier()\n",
        "naive_bayes = GaussianNB()\n",
        "gradient_boosting = GradientBoostingClassifier()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef1Blt6tnl3J"
      },
      "source": [
        "In NumPy, the reshape function is used to change the shape of an array without changing its data. The arguments (-1, 1) and (1, -1) specify how the array should be reshaped. Here’s a detailed explanation of each:\n",
        "\n",
        "reshape(-1, 1)\n",
        "-1: This is a special placeholder used in NumPy’s reshape method. It tells NumPy to automatically determine the size of this dimension based on the size of the array and the remaining dimensions.\n",
        "1: This specifies that the resulting shape should have a single column.\n",
        "\n",
        "\n",
        "Explanation: The -1 tells NumPy to infer the number of rows based on the total number of elements (which is 6 in this case) and the specified number of columns (1). So, the resulting shape is (6, 1).\n",
        "\n",
        "reshape(1, -1)\n",
        "1: This specifies that the resulting shape should have a single row.\n",
        "-1: This tells NumPy to automatically determine the size of this dimension based on the size of the array and the remaining dimensions.\n",
        "\n",
        "\n",
        "Explanation: The -1 tells NumPy to infer the number of columns based on the total number of elements (which is 6 in this case) and the specified number of rows (1). So, the resulting shape is (1, 6).\n",
        "\n",
        "Summary\n",
        "reshape(-1, 1) converts a 1D array into a 2D array with one column and as many rows as needed.\n",
        "reshape(1, -1) converts a 1D array into a 2D array with one row and as many columns as needed.\n",
        "The -1 in the reshape function is useful for automatically calculating dimensions when you only need to specify one of the dimensions, making it easier to reshape arrays without manually calculating the required sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to track the cpu, gpu and memory usage of the algorithm, using a decorator, then using mlflow to track the logs."
      ],
      "metadata": {
        "id": "D7X9Q5md5Esn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using different models for training and evaluating the model."
      ],
      "metadata": {
        "id": "R-izXSXWkIxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, Xtrain: np.ndarray, ytrain:np.ndarray):\n",
        "    # Train the model\n",
        "    model.fit(Xtrain, ytrain)\n"
      ],
      "metadata": {
        "id": "pmmnEm_RP7-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, Xtest: np.ndarray)-> None:\n",
        "    predictions = model.predict(Xtest)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "VM52RXEUad_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, y_test: np.ndarray, y_pred: np.ndarray):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Model: {model.__class__.__name__}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model.__class__.__name__}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "KrVNYIP1QDeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(model, Xtrain, ytrain, Xtest, ytest):\n",
        "    train_model(model, Xtrain, ytrain)\n",
        "    y_pred = predict(model, Xtest)\n",
        "    evaluate_model(model, ytest, y_pred)"
      ],
      "metadata": {
        "id": "JbhFvvwfXtYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, y_pred, model_performance  = main(logistic_regression, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "h_lGB77SDeal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#save model as pickle file\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    with open(model_name, 'wb') as f:\n",
        "        pickle.dump(model, f)\n"
      ],
      "metadata": {
        "id": "0DEaBR4-QTvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "\n",
        "def load_model(model_name):\n",
        "    with open(model_name, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    return model"
      ],
      "metadata": {
        "id": "vD9ahSo_jfAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Decision Tree\n",
        "#def decision_tree_model(Xtrain, ytrain, Xtest):\n",
        "    #train_model(decision_tree, Xtrain, ytrain, Xtest)"
      ],
      "metadata": {
        "id": "hCjJtQd9nVI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decision_tree_model(Xtrain, ytrain, Xtest)"
      ],
      "metadata": {
        "id": "Mtm1s8IAQ4ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decision_tree_model(Xtrain, ytrain, Xtest, ytest)"
      ],
      "metadata": {
        "id": "5wCqh3ltnaoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtQVT6HMndpU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFORY916ndyK"
      },
      "outputs": [],
      "source": [
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOB7xJ9wUOxn"
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oQPyoN-77RQTUpyA3tD2R4kvQ5fiUIx3",
      "authorship_tag": "ABX9TyPQNt+vmqbMo2F+rKjNV0Pm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}