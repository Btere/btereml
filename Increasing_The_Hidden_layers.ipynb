{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5wWr0UN0gK0e+QRqfzNWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Btere/btereml/blob/main/Increasing_The_Hidden_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8IuIK-z_0oY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "\n",
        "%matplotlib inline\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(root='path/to/dataset', train = True, download = True)\n",
        "tr_images = train_data.data\n",
        "tr_targets = train_data.targets"
      ],
      "metadata": {
        "id": "REC8btjqAStE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.MNIST(root='path/to/dataset', train =False, download =True)\n",
        "val_images = test_data.data\n",
        "val_targets = test_data.targets"
      ],
      "metadata": {
        "id": "WTlY7xiTASwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.classes"
      ],
      "metadata": {
        "id": "0_r5rS7zASzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=tr_images[39999,:,:].shape"
      ],
      "metadata": {
        "id": "Fsf46fPsAS2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 5, 5\n",
        "\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
        "    img, label = train_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(np.squeeze(img), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FyLNXXzFAS46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flatten the image:\n",
        "flattening is used to reduce the dimensionality of the input to a layer. A dense layer expects a row vector which we could say is convenient equivalent of Numpy's reshape. It allows us to do fast and memory efficient reshaping, slicing and element-wise operations."
      ],
      "metadata": {
        "id": "_f5BO3FzAp8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing of the data.\n",
        "class Mnist_dataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        x = x.float()\n",
        "        x = x.view(-1,28*28)\n",
        "        self.x, self.y = x, y\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        x, y = self.x[ix], self.y[ix]\n",
        "        return x.to(device), y.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n"
      ],
      "metadata": {
        "id": "xfpkg136Aj-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    train = Mnist_dataset(tr_images, tr_targets)\n",
        "    trn_dl = DataLoader(train, batch_size=1000, shuffle=True)\n",
        "    val = Mnist_dataset(val_images, val_targets)\n",
        "    val_dl = DataLoader(val, batch_size=len(val_images), shuffle=False)\n",
        "    return trn_dl, val_dl"
      ],
      "metadata": {
        "id": "SKOhzRUGAkGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building The custom Network"
      ],
      "metadata": {
        "id": "Xm_JIWlQBHCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "def build_model():\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(28 * 28, 900),     #torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(900, 900),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(900,900),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(900, 10)            # label =10, node in the hidden layer that is passed to output layer =1000\n",
        "    ).to(device)\n",
        "    #Defining loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=1e-2)\n",
        "    return model, loss_fn, optimizer"
      ],
      "metadata": {
        "id": "Hn4kXWVdAkKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_model()"
      ],
      "metadata": {
        "id": "7AS62hY2AkPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the network\n",
        "Here, we train the image classification model. Epoch: Is a single pass through the training data (60,000 images). The number of batches passed to the model until all the training data is covered."
      ],
      "metadata": {
        "id": "uA3ISZlcCX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The training loop\n",
        "#we used .item() to EXTRACTTHE LOSS VALUE AS A SCALER.\n",
        "def train_batch(x, y, model, optimizer, loss_fn):\n",
        "      model.train()\n",
        "      prediction = model(x)\n",
        "      batch_loss = loss_fn(prediction, y)\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      return batch_loss.item()\n",
        "\n",
        "def accuracy(x, y, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(x)\n",
        "    max_values, argmaxes = prediction.max(-1)\n",
        "    is_correct = argmaxes == y\n",
        "    return is_correct.cpu().numpy().tolist()"
      ],
      "metadata": {
        "id": "6ZwLtDPqCVMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def val_loss(x, y, model):\n",
        "    prediction = model(x)\n",
        "    val_loss = loss_fn(prediction, y)\n",
        "    return val_loss.item()"
      ],
      "metadata": {
        "id": "lyuWZsmPCVRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_dl, val_dl = get_data()\n",
        "model, loss_fn, optimizer = build_model()"
      ],
      "metadata": {
        "id": "rYy-u420CVaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "for epoch in range(5):\n",
        "    print(epoch)\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        batch_loss = train_batch(x, y, model, optimizer, loss_fn)\n",
        "        train_epoch_losses.append(batch_loss)\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "\n",
        "    for ix, batch in enumerate(iter(trn_dl)):\n",
        "        x, y = batch\n",
        "        is_correct = accuracy(x, y, model)\n",
        "        train_epoch_accuracies.extend(is_correct)\n",
        "    train_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
        "\n",
        "    for ix, batch in enumerate(iter(val_dl)):\n",
        "        x, y = batch\n",
        "        val_is_correct = accuracy(x, y, model)\n",
        "        validation_loss = val_loss(x, y, model)\n",
        "    val_epoch_accuracy = np.mean(val_is_correct)\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    train_accuracies.append(train_epoch_accuracy)\n",
        "    val_losses.append(validation_loss)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n"
      ],
      "metadata": {
        "id": "9KxuaVnmCVdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(5)+1\n",
        "import matplotlib.ticker as mtick\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "%matplotlib inline\n",
        "plt.subplot(211)\n",
        "plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
        "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.title('Training and validation loss when batch size is 1000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()\n",
        "plt.subplot(212)\n",
        "plt.plot(epochs, train_accuracies, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracies, 'r', label='Validation accuracy')\n",
        "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "plt.title('Training and validation accuracy when batch size is 1000')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
        "plt.legend()\n",
        "plt.grid('off')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "2-BSZXFdCVgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GIMVNrVoHNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3plTXNzoHLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlt03n2VoG-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KM5rj8CFoG8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alzcbShpoG4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3s-5eYkoG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JmTSFzcxoGZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nGlnYvstoGU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tfrWyb0oGPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g30v9LdWoGLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QQMUpk4oGIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TaTWrf54oGEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSw7Ot99oGBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHClQzKAoF-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4b4j-W98oF7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4qOyKdaoF4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNaDGOA9oF0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Analysis of Result\n",
        "# Red: VL\n",
        "#Blue TL\n",
        "\n",
        "#During training, the  model's parameter are updated in an attempt to increase the accuracy on both the training and validation data.\n",
        "# IT is important to  monitor the validation accuracy to ensure that the model is not overfitting the training data.\n",
        "# The training accuracy is the fraction of correctly classified examples in the training data. A high  accuracy indicate that the  model is\n",
        "# making  good predictions on the training data.\n",
        "# training loss: At the beginning, the training loss is typically high as the model starts with random weights. As the training progresses, the loss should decrease, indicating that the model is learning from the data. If the loss does not decrease,\n",
        "#then what happens or what does it implies?\n",
        "#Validation loss is when  the validation loss increase probably at first or decreases cos the model has seen similar data before, and with more training it does not increases.\n",
        "# WHen there is overfitting, the training loss decreases, and and validation loss increases\n",
        "\n",
        "# Validation Accuracy: The  validation accuracy is the fraction of correctly classifed examples in the validation data.\n",
        "\n",
        "#Trained data, find the loss and accuracy. Then we passed a test data, check the loss(the error) and check the accuracy(the fraction of  correctly)\n",
        "# classifed dataset.\n",
        "# # Traning loss vs validation loss and training accuracy vs validation accuracy.\n",
        "# The graph of training and validation loss in the context of machine learning represents how well a model is performing during the training process. Here's what each term means:\n",
        "\n",
        "# 1. **Training Loss:**\n",
        "#    - **Definition:** The training loss is a measure of how well the model is performing on the training dataset. It represents the error between the predicted values and the actual values during the training phase.\n",
        "#    - **Graph Behavior:** In the beginning, the training loss is typically high as the model starts with random weights. As the training progresses, the loss should decrease, indicating that the model is learning from the data.\n",
        "\n",
        "# 2. **Validation Loss:**\n",
        "#    - **Definition:** The validation loss is a measure of how well the model is performing on a separate dataset that it has not seen during training. This dataset, called the validation set, serves as a proxy for new, unseen data.\n",
        "#    - **Graph Behavior:** The validation loss is crucial for detecting overfitting. Initially, it might decrease along with the training loss. However, if the model starts memorizing the training data too much and loses its ability to generalize, the validation loss may start to increase even as the training loss continues to decrease.\n",
        "\n",
        "# Here's a breakdown of the possible scenarios based on the graph:\n",
        "\n",
        "# - **Ideal Scenario:**\n",
        "#   - Both training and validation losses decrease steadily over time.\n",
        "#   - This indicates that the model is learning from the training data and generalizing well to new, unseen data.\n",
        "\n",
        "# - **Overfitting Scenario:**\n",
        "#   - Training loss decreases, but validation loss increases.\n",
        "#   - This suggests that the model is fitting the training data too closely and is not able to generalize well to new data.\n",
        "\n",
        "# - **Underfitting Scenario:**\n",
        "#   - Both training and validation losses remain high.\n",
        "#   - The model is not able to capture the patterns in the training data, and it performs poorly on both the training and validation sets.\n",
        "\n",
        "# Monitoring these loss curves is a common practice in training machine learning models to ensure they are learning effectively without overfitting or underfitting.\n",
        "#When we iterate the first epoch, what happen to the loss function? When we update the  the weight, on each epoch what happen to the\n",
        "\n",
        "#we train, predict with xtest, then find accuracy\n",
        "\n",
        "# We have 60,0000 DS, we grouped(batch-size) = 1000, How many batches complete one epoch? 100\n",
        "\n",
        "\n",
        "# check batch optimzation and mini batch training.\n"
      ],
      "metadata": {
        "id": "VZ3T3_KOCVjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfitting is a common issue in machine learning where a model learns the training data too well, including its noise and random fluctuations, to the extent that it negatively impacts the model's performance on new, unseen data. In other words, an overfit model performs well on the training data but fails to generalize effectively to new, unseen data.\n",
        "\n",
        "# Key characteristics of overfitting:\n",
        "\n",
        "# 1. **High Training Accuracy, Poor Generalization:**\n",
        "#    - The model achieves high accuracy on the training dataset because it memorizes the training examples.\n",
        "#    - However, when presented with new data (validation or test set), its performance is significantly worse.\n",
        "\n",
        "# 2. **Complex Model:**\n",
        "#    - Overfitting often occurs when the model is too complex, with too many parameters or features relative to the size of the training dataset.\n",
        "\n",
        "# 3. **Capturing Noise:**\n",
        "#    - The model captures not only the underlying patterns in the data but also noise, random fluctuations, or outliers present in the training data.\n",
        "\n",
        "# 4. **Poor Generalization:**\n",
        "#    - The overfit model fails to generalize well to new, unseen data, leading to poor performance in real-world scenarios.\n",
        "\n",
        "# 5. **Validation Loss Plateau or Increase:**\n",
        "#    - In the training-validation loss curve, the training loss may continue to decrease, but the validation loss either plateaus or starts to increase, indicating that the model is not improving on new data.\n",
        "\n",
        "# Methods to address overfitting:\n",
        "\n",
        "# 1. **Simplifying the Model:**\n",
        "#    - Reduce the complexity of the model by reducing the number of parameters, features, or layers.\n",
        "\n",
        "# 2. **Regularization:**\n",
        "#    - Add regularization techniques like L1 or L2 regularization to penalize overly complex models.\n",
        "\n",
        "# 3. **Dropout:**\n",
        "#    - Use dropout, a regularization technique where randomly selected neurons are ignored during training to prevent reliance on specific neurons.\n",
        "\n",
        "# 4. **More Data:**\n",
        "#    - Increase the size of the training dataset to provide more diverse examples for the model to learn from.\n",
        "\n",
        "# 5. **Cross-Validation:**\n",
        "#    - Use techniques like cross-validation to assess the model's performance on multiple subsets of the data.\n",
        "\n",
        "# Overfitting is a crucial challenge in machine learning, and finding the right balance between model complexity and generalization is essential for building effective models."
      ],
      "metadata": {
        "id": "kKWeBkVJCVlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understanding APi, API call and response or errors tht could occur.\n",
        "#Absolutely! Your analogy with reading the Bible and making an API call through prayers is a creative way to understand the concept.\n",
        "\n",
        "# In the same way, when you interact with an API to get information from a website or a third-party service:\n",
        "\n",
        "# - **Reading the Bible:** This is like having access to the API documentation. It's a guide that tells you what kind of requests (prayers) you can make and what information (answers) you can expect.\n",
        "\n",
        "# - **API Call through Prayers:** When you make an API call, it's like saying a prayer. You're expressing a specific request based on what the API documentation (Bible) allows.\n",
        "\n",
        "# - **Response:** Just like you trust that your prayers will be answered, you trust that the API will send back a response. This response could be the information you requested or an indication that there's an issue (just like how you might sense something is wrong if your prayer isn't answered promptly).\n",
        "\n",
        "# - **Checking Again:** If the response takes longer than expected or if there's an error, you might \"check in\" again with another prayer. In the tech world, this could involve reattempting the API call or troubleshooting any issues.\n",
        "\n",
        "# So, the idea is similarâ€”there's a communication process where you express a specific need, trust that it will be addressed, and handle any issues that might arise in the interaction. It's a great way to connect a technical concept with something more familiar and personal!"
      ],
      "metadata": {
        "id": "3CeebwNQm9Nj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}